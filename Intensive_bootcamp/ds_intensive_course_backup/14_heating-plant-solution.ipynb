{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EePx5wNWKAq"
      },
      "source": [
        "# Heating plant\n",
        "\n",
        "We have a dataset from a real heating plant located in a medium-sized city in Europe. The heating plant heats water and distributes the heat around the city. Our goal is to predict temperature of the returning water based on actual and historical power settings of the plant, output water temperature and the outside temperatures measured at different locations in the city.\n",
        "\n",
        "## Approach to solution\n",
        "- We will attempt to model the progression as separate time series predictions\n",
        "- If we succeed in modeling the *temp_in*, we will attempt the same for the other variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axsWXirEWO5B"
      },
      "source": [
        "## Data import\n",
        "- we will consolidate the dataset in one data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tczeX-9WQvp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "frames = []\n",
        "for i in range (0,10):\n",
        "    frames.append(pd.read_excel(\"power_plant.xlsx\", sheet_name=i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = frames[0][['ts','power12']].merge(frames[1][['ts','power3']], on='ts').merge(frames[2][['ts','power4']], on='ts')\n",
        "data = data.merge(frames[3][['ts','temp1']], on='ts')\n",
        "data = data.merge(frames[4][['ts','temp2']], on='ts')\n",
        "data = data.merge(frames[5][['ts','temp3']], on='ts')\n",
        "data = data.merge(frames[6][['ts','temp4']], on='ts')\n",
        "data = data.merge(frames[7][['ts','temp5']], on='ts')\n",
        "data = data.merge(frames[8][['ts','temp_in']], on='ts')\n",
        "data = data.merge(frames[9][['ts','temp_out']], on='ts')\n",
        "\n",
        "data['ts'] = pd.to_datetime(data['ts'])\n",
        "backup_frame = data.copy()\n",
        "data.set_index('ts', inplace=True)\n",
        "#data.index.freq = 'H'\n",
        "\n",
        "px.scatter(data[\"temp_in\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_interpolated = data.resample('H').interpolate(method='linear')  # Example for hourly interpolation\n",
        "px.scatter(data_interpolated[\"temp_in\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outlier filtering\n",
        "The data is not clean, we have to solve a couple of relatively big issues:\n",
        "1. Obviously wrong measurements in the *temp_in* data\n",
        "2. The time stamps are not regular (missing measurements)\n",
        "\n",
        "**Solutions**\n",
        "1. Perform some kind of outlier filtering (Statistical - Standard deviation or Interquartile range / Logical - hard clip if we have a hypothesis)\n",
        "2. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_std(df, column):\n",
        "    mean_temp = df[column].mean()\n",
        "    std_temp = df[column].std()\n",
        "    cutoff = std_temp * 3  # or 2, depending on how strict you want to be\n",
        "    lower, upper = mean_temp - cutoff, mean_temp + cutoff\n",
        "    return lower, upper\n",
        "\n",
        "def filter_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
        "    return lower, upper\n",
        "\n",
        "# Calculate lower and upper bounds for outlier filtering\n",
        "\n",
        "# HYPOTHESIS\n",
        "# The wrong measurements are caused by malfunctions of sensors,\n",
        "# therefore we will erase the most obvious mistakes by performing\n",
        "# a hard clip based on the time series progression.\n",
        "\n",
        "# You may play with the lower and upper bounds\n",
        "# a bit to explore how the filtered data looks like.\n",
        "\n",
        "#lower, upper = filter_std(data_interpolated, \"temp_in\")\n",
        "lower = 40.\n",
        "upper = 80.\n",
        "\n",
        "# Perform filtering\n",
        "data_filtered = data_interpolated.copy()\n",
        "\n",
        "# Apply transformation\n",
        "#data_filtered['temp_in'] = data_filtered['temp_in'].apply(lambda x: min(max(x, lower), upper))\n",
        "\n",
        "# Remove data outside the bounds and replace with linear interpolation\n",
        "mask = (data_filtered['temp_in'] >= lower) & (data_filtered['temp_in'] <= upper)\n",
        "data_filtered = data_filtered[mask]\n",
        "#data_filtered = data_filtered.interpolate(method='linear')\n",
        "data_filtered = data_filtered.asfreq('H')  # Replace 'H' with your data's frequency\n",
        "data_filtered['temp_in'].interpolate(method='linear', inplace=True)\n",
        "\n",
        "missing_values = data_filtered['temp_in'].isnull().sum()\n",
        "print(f\"Number of missing values: {missing_values}\")\n",
        "\n",
        "# Visualize\n",
        "fig = px.scatter(data_filtered[\"temp_in\"])\n",
        "fig.add_hline(y=lower, line_dash=\"dash\", line_color=\"red\")\n",
        "fig.add_hline(y=upper, line_dash=\"dash\", line_color=\"red\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filtering aftermath\n",
        "Now that we have performed filtering and managed to insert the missing timestamps, now we can deal with the usual time series workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "decomposition = sm.tsa.seasonal_decompose(data_filtered['temp_in'], model='multiplicative',period=2)  # or model='multiplicative'\n",
        "\n",
        "trend = decomposition.trend\n",
        "seasonal = decomposition.seasonal\n",
        "residual = decomposition.resid\n",
        "\n",
        "fig = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.02)\n",
        "# Trend\n",
        "fig.add_trace(go.Scatter(x=trend.index, y=trend, mode='lines', name='Trend'), row=1, col=1)\n",
        "# Seasonal\n",
        "fig.add_trace(go.Scatter(x=seasonal.index, y=seasonal, mode='lines', name='Seasonal'), row=2, col=1)\n",
        "# Residual\n",
        "fig.add_trace(go.Scatter(x=residual.index, y=residual, mode='lines', name='Residual'), row=3, col=1)\n",
        "# Settings\n",
        "fig.update_layout(height=600, width=800, title_text=\"Time Series Decomposition\")\n",
        "fig.update_xaxes(title_text=\"Time\", row=3, col=1)\n",
        "fig.update_yaxes(title_text=\"Trend\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Seasonality\", row=2, col=1)\n",
        "fig.update_yaxes(title_text=\"Residual\", row=3, col=1)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decomposition interpretation\n",
        "- Right off the bat, we can see something strange in the seasonal component. This kind of plot suggests that there might be no seasonal process at all!\n",
        "- There is also nothing in the residuals.\n",
        "- Let's inspect the stationarity and autocorrelation to get some more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "result = adfuller(data_filtered['temp_in'])\n",
        "print('ADF Statistic: %f' % result[0])\n",
        "print('p-value: %f' % result[1])\n",
        "\n",
        "# Interpret the p-value\n",
        "if result[1] > 0.05:\n",
        "    print(\"Series is not stationary\")\n",
        "else:\n",
        "    print(\"Series is stationary\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The series is stationary according to ADF test then. That result could be expected based on the character of the *temp_in* progression.\n",
        "\n",
        "# Plot ACF and PACF\n",
        "We are looking for the first lag value where the ACF and PACF respectively are inside the confidence interval (blue area)\n",
        "- PACF indicates the candidate for optimal *p* in the ARIMA model.\n",
        "- ACF indicates the candidate *q*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plot_acf(data_filtered['temp_in'])\n",
        "plot_pacf(data_filtered['temp_in'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ACF & PACF interpretation\n",
        "- PACF indicates that the optimal $q = 3$\n",
        "- The ACF plot however is problematic. The series autocorrelates with itself perfectly. That could mean a lot of things, but among them that there *might* be no underlying seasonal processes or patterns at all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "# ARIMA definition\n",
        "p, d, q = 4, 0, 2\n",
        "model = ARIMA(data_filtered['temp_in'], order=(p, d, q))\n",
        "model_fit = model.fit()\n",
        "print(model_fit.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ARIMA Summary\n",
        "The model summary does not look good.\n",
        "- the AIC and BIC are enormous. If you experiment with $p,d,q$, you may notice that they stay roughly the same level of disgusting.\n",
        "- But our hopes still may be warranted, although the values are enormous, the model may yet work... Let's try to construct the series using forecasting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "y_all = data_filtered[\"temp_in\"]\n",
        "y_all = np.array(y_all)\n",
        "\n",
        "train_size = int(len(y_all) * 0.8)  # 80% of data used for training\n",
        "train, test = y_all[:train_size], y_all[train_size:]\n",
        "\n",
        "# Generate ARIMA predictions using the fitted model and its parameters\n",
        "predictions = model_fit.forecast(steps=len(test))\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "df = pd.DataFrame({\n",
        "    'Actual': test,\n",
        "    'Predicted': predictions,\n",
        "    'Time': range(len(test))\n",
        "})\n",
        "\n",
        "# Calculate the difference\n",
        "df['Difference'] = df['Actual'] - df['Predicted']\n",
        "\n",
        "# Create a line plot for actual vs predicted\n",
        "fig = px.line(df, x='Time', y=['Actual', 'Predicted'])\n",
        "\n",
        "# Add a bar plot for the difference\n",
        "fig.add_bar(x=df['Time'], y=df['Difference'], name='Difference')\n",
        "\n",
        "# Update layout for clarity\n",
        "fig.update_layout(\n",
        "    title=\"ARIMA Model Predictions vs Actual\",\n",
        "    xaxis_title=\"Time\",\n",
        "    yaxis_title=\"Values\",\n",
        "    legend_title=\"Legend\",\n",
        "    barmode='overlay'  # Overlays the bar plot on the line plot\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lessons learned\n",
        "- Our series was stationary according to the ADF test\n",
        "- We had decent partial autocorrelation (PACF)\n",
        "- the ACF was somewhat sketchy\n",
        "- Decomposition only indicated that the series might not be seasonal, but that did not yet point to the series being unforecastable\n",
        "\n",
        "Based on the result above, however, the model is an **obvious failure**.\n",
        "\n",
        "#### Why is that?\n",
        "The forecast has basically the shape of a series of mean values across the time frame. \n",
        "The seasonal elements us humans can kinda see behind the process (the oscillations during summer and winter) are hidden in the noise. To extract the underlying patterns, ranging from seasonality to direct inference from the other variables, we need to leverage the dependencies between them, and that is what we cannot do with (S)ARIMA(X) or simple LSTM's.\n",
        "\n",
        "#### Key takeaways\n",
        "This task was intended as a rather rough troubleshooting task, rather than a straightforward modeling assignment.\n",
        "You were supposed to detect the missing timestamps, and decide how to deal with\n",
        "1. The missing time stamps themselves, as you cannot fit them using either ARIMA or LSTM.\n",
        "2. Once you inserted timestamps to adhere to the precribed frequency, how to deal with missing values of the series and outliers.\n",
        "If you managed to get thus far and attempt to fit the ARIMA model, you are absolutely fantastic! It was a really hard assignment.\n",
        "The straightforward time series prediction is just **not enough**.\n",
        "\n",
        "As far as true solution goes, ARIMA is not the way. \n",
        "There are many ways to continue, if you had to persevere and find an approach which can serve at least as a baseline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Alternative approach\n",
        "\n",
        "**Idea:** If we cut the time series into shorter segments, we may view the problem as static, thus time series modeling might be completely avoided!\n",
        "\n",
        "We will thus perform a standard regression.\n",
        "- This leverages the other variables in the prediction of the *temp_in*\n",
        "- By performing more inferences over smaller time frames, we have a relatively high chance of accurately catching the time dependency implicitly..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from datetime import timedelta\n",
        "\n",
        "def rmse(predictions, true_values):\n",
        "    return np.sqrt(mean_squared_error(predictions, true_values))\n",
        "\n",
        "# We go back to the original dataset (data)\n",
        "bf = backup_frame\n",
        "\n",
        "#identify the gaps in the data that cause the trouble\n",
        "bf['gap'] = (bf['ts'] - bf['ts'].shift(1)) != timedelta(hours=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I am going to segment the original data frame into more granular data frames, and we will use these for training our baseline model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a list of data frames based on the gap positions\n",
        "attributes = ['power12', 'power3', 'power4', 'temp1', 'temp2', 'temp3', 'temp4', 'temp5', 'temp_in', 'temp_out']\n",
        "\n",
        "dflist = []\n",
        "\n",
        "start = 0\n",
        "for stop in range(1, len(bf)):\n",
        "    if bf.iloc[stop]['gap']:\n",
        "        dflist.append(bf[start:stop][attributes])\n",
        "        start = stop\n",
        "len(dflist)\n",
        "\n",
        "w = 5 #window size\n",
        "s = 1 #step\n",
        "X_all = []\n",
        "y_all = []\n",
        "\n",
        "for df in dflist:\n",
        "    for i in range(0, len(df)-w-1, s):\n",
        "        X_all.append(df[i:i+w].values)\n",
        "        y_all.append(df.iloc[i+w]['temp_in'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_all = np.array(X_all)\n",
        "y_all = np.array(y_all)\n",
        "\n",
        "print (X_all.shape)\n",
        "print (y_all.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "\n",
        "mean_baseline = []\n",
        "last_values_baseline = []\n",
        "ml_model = []\n",
        "iters = 20\n",
        "train_size = int(0.9*len(X_all))\n",
        "\n",
        "X_all_flat = X_all.reshape(X_all.shape[0], -1)\n",
        "y_all_baseline = X_all_flat[:, -2]\n",
        "\n",
        "for i in range(iters):\n",
        "    all_data = list(zip(X_all_flat, y_all, y_all_baseline))\n",
        "    random.shuffle(all_data)\n",
        "    X_all_flat_rand, y_all_rand, y_all_rand_baseline = zip(*all_data)\n",
        "    X_train = np.array(X_all_flat_rand[:train_size])\n",
        "    y_train = np.array(y_all_rand[:train_size])\n",
        "    X_test = np.array(X_all_flat_rand[train_size:])\n",
        "    y_test = np.array(y_all_rand[train_size:])\n",
        "    y_baseline = np.array(y_all_rand_baseline[train_size:])\n",
        "    \n",
        "    print (\"Training iteration {}.\".format(i+1))\n",
        "    regr = GradientBoostingRegressor(n_estimators=500)\n",
        "    regr.fit(X_train, y_train)\n",
        "    y_pred = regr.predict(X_test)\n",
        "    \n",
        "    m = np.mean(y_train)\n",
        "    y_mean = np.array([m for i in range(len(y_test))])\n",
        "    \n",
        "    mean_baseline.append(rmse(y_mean, y_test))\n",
        "    last_values_baseline.append(rmse(y_baseline, y_test))\n",
        "    ml_model.append(rmse(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print (\"Mean baseline: {} +- {}.\".format(np.mean(mean_baseline), np.std(mean_baseline)))\n",
        "print (\"Last values baseline: {} +- {}.\".format(np.mean(last_values_baseline), np.std(last_values_baseline)))\n",
        "print (\"ML model: {} +- {}.\".format(np.mean(ml_model), np.std(ml_model)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Mean baseline: 6.280885842437059 +- 0.4750540060777046.\n",
        "- Last values baseline: 1.4129028636032352 +- 0.40922030443042745.\n",
        "- ML model: 1.3960461889178457 +- 0.34344185316048037.\n",
        "\n",
        "Based on the results, the GBR approach is quite promising.\n",
        "We could tweak the time frames and try to isolate seasonal components using more thorough segmentation.\n",
        "\n",
        "Alternatively, we could try augmenting time series modeling with regular regression, or other advanced techniques.\n",
        "The options are really endless."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Heating-plant-assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
